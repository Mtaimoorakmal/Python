{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2195f8fa",
   "metadata": {},
   "source": [
    "We receive a .txt file from a vendor, but the content is comma-separated values. How do you process it?‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b10e6c",
   "metadata": {},
   "source": [
    "Solution 1: Read using Python csv module (BEST PRACTICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18bb5a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'order_id': '101', 'customer': 'Alice', 'amount': '250', 'order_date': '2024-01-01'}\n",
      "{'order_id': '102', 'customer': 'Bob', 'amount': '400', 'order_date': '2024-01-02'}\n",
      "{'order_id': '103', 'customer': 'Charlie', 'amount': '150', 'order_date': '2024-01-03'}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"orders.txt\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "\n",
    "    for row in reader:\n",
    "        print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211b176",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "881f5c31",
   "metadata": {},
   "source": [
    "Solution 2: Using pandas (Most common in DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55975bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id customer  amount  order_date\n",
      "0       101    Alice     250  2024-01-01\n",
      "1       102      Bob     400  2024-01-02\n",
      "2       103  Charlie     150  2024-01-03\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"orders.txt\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0735cf8f",
   "metadata": {},
   "source": [
    "Level 2 ‚Äî File extension is wrong AND delimiter is weird"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f0df8",
   "metadata": {},
   "source": [
    "Answer: Delimiter mismatch is common; I inspect sample rows and set delimiter explicitly.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59f181a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   txn_id user  amount   status\n",
      "0       1    A     500  success\n",
      "1       2    B     300   failed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"transactions.txt\", delimiter=\"|\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d21ab",
   "metadata": {},
   "source": [
    "Level 3 ‚Äî File has extra junk lines (VERY COMMON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6816b4",
   "metadata": {},
   "source": [
    "Scenario\n",
    "\n",
    "Vendor adds metadata before actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55ec123",
   "metadata": {},
   "source": [
    "Solution: Skip rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12c0a415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  amount\n",
      "0         1     100\n",
      "1         2     200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sales.txt\", skiprows=3)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bad382a",
   "metadata": {},
   "source": [
    "\n",
    "üîπ Level 4 ‚Äî File has NO extension at all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc7145",
   "metadata": {},
   "source": [
    "Scenario\n",
    "\n",
    "Data comes as data_dump (no .csv, no .txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd616d",
   "metadata": {},
   "source": [
    "Detect content automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d31757f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txn_id|user|amount|status\n",
      "\n",
      "Looks like pipe delimited\n"
     ]
    }
   ],
   "source": [
    "with open(\"data_dump\", \"r\") as f:\n",
    "    first_line = f.readline()\n",
    "\n",
    "print(first_line)\n",
    "\n",
    "\n",
    "if \",\" in first_line:\n",
    "    print(\"Looks like CSV\")\n",
    "elif \"|\" in first_line:\n",
    "    print(\"Looks like pipe delimited\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8779cc",
   "metadata": {},
   "source": [
    "Level 5 ‚Äî CSV inside .txt but values have commas (üî• tricky)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6151dc5",
   "metadata": {},
   "source": [
    "Correct handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97397b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   name             address\n",
      "0   1  Alice  Street 1, New York\n",
      "1   2    Bob    Street 2, London\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"customers.txt\", quotechar='\"')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59c2718",
   "metadata": {},
   "source": [
    "Level 6 ‚Äî Mixed bad rows (real production pain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d32da5",
   "metadata": {},
   "source": [
    "Scenario\n",
    "\n",
    "Some rows are corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6484dd3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216e3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id amount\n",
      "0   1    100\n",
      "1   2    ABC\n",
      "2   3    NaN\n",
      "3   3    300\n"
     ]
    }
   ],
   "source": [
    "# Handle safely\n",
    "# the file is like \n",
    "'''\n",
    "id,amount\n",
    "1,100\n",
    "2,ABC\n",
    "3,\n",
    "4,,    ------->badline will be skiped/warned/errored\n",
    "3,300\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sales_with_badlines.txt\", on_bad_lines=\"skip\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda6e42",
   "metadata": {},
   "source": [
    "PART 2 ‚Äî Read file safely using CHUNKS (Production way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c5c20",
   "metadata": {},
   "source": [
    "Interview scenario\n",
    "\n",
    "‚ÄúHow do you process a large file with bad rows without crashing?‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5e1892b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9979, 4)\n",
      "  order_id customer  amount  order_date\n",
      "0        1    David   581.0  2024-07-12\n",
      "1        2  Charlie   658.0  2024-09-08\n",
      "2        3    David   547.0  2024-01-29\n",
      "3        4    Alice   723.0  2024-12-30\n",
      "4        5    David   677.0  2024-10-16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_size = 1000\n",
    "valid_rows = []\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    \"large_orders.txt\",\n",
    "    chunksize=chunk_size,\n",
    "    on_bad_lines=\"skip\"\n",
    "    # on_bad_lines=\"error\"\n",
    "    # on_bad_lines=\"warn\"\n",
    "):\n",
    "    # Convert amount safely\n",
    "    chunk[\"amount\"] = pd.to_numeric(chunk[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows where amount is invalid\n",
    "    clean_chunk = chunk.dropna(subset=[\"amount\"])\n",
    "\n",
    "    valid_rows.append(clean_chunk)\n",
    "    # valid_rows.append(chunk)\n",
    "\n",
    "final_df = pd.concat(valid_rows, ignore_index=True)\n",
    "\n",
    "print(final_df.shape)\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739bd20",
   "metadata": {},
   "source": [
    "PART 3 ‚Äî Log bad rows separately (VERY IMPRESSIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa7dcc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad rows saved: (21, 4)\n"
     ]
    }
   ],
   "source": [
    "bad_rows = []\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    \"large_orders.txt\",\n",
    "    chunksize=1000,\n",
    "    on_bad_lines=\"skip\"\n",
    "):\n",
    "    chunk[\"amount\"] = pd.to_numeric(chunk[\"amount\"], errors=\"coerce\")\n",
    "\n",
    "    invalid = chunk[chunk[\"amount\"].isna()]\n",
    "    bad_rows.append(invalid)\n",
    "\n",
    "bad_df = pd.concat(bad_rows, ignore_index=True)\n",
    "bad_df.to_csv(\"bad_rows.csv\", index=False)\n",
    "\n",
    "print(\"Bad rows saved:\", bad_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bca004",
   "metadata": {},
   "source": [
    "PART 4 ‚Äî Edge Scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be2724d",
   "metadata": {},
   "source": [
    "File suddenly changes schema (column added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12a9b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_cols = {\"order_id\", \"customer\", \"amount\", \"order_date\"}\n",
    "\n",
    "for chunk in pd.read_csv(\"large_orders.txt\", chunksize=1000):\n",
    "    if set(chunk.columns) != expected_cols:\n",
    "        raise ValueError(\"Schema mismatch detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56b2fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is empty, skipping processing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.getsize(\"empty.txt\") == 0:\n",
    "    print(\"File is empty, skipping processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9965ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = final_df.drop_duplicates(subset=[\"order_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "86af9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"large_orders.txt\", \"rb\") as f:\n",
    "    content = f.read().replace(b\"\\x00\", b\"\")\n",
    "\n",
    "with open(\"cleaned.txt\", \"wb\") as f:\n",
    "    f.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c587796f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
